{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch datasets transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wbOefao75gX",
        "outputId": "5e43a583-ac33-42ed-a150-fc303608c1db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZeyBl4KY42Ir"
      },
      "outputs": [],
      "source": [
        "pairs_en_fr = [\n",
        "\n",
        "    (\"I am cold\", \"J'ai froid\"),\n",
        "    (\"You are tired\", \"Tu es fatigué\"),\n",
        "    (\"He is hungry\", \"Il a faim\"),\n",
        "    (\"She is happy\", \"Elle est heureuse\"),\n",
        "    (\"We are friends\", \"Nous sommes amis\"),\n",
        "    (\"They are students\", \"Ils sont étudiants\"),\n",
        "    (\"The cat is sleeping\", \"Le chat dort\"),\n",
        "    (\"The sun is shining\", \"Le soleil brille\"),\n",
        "    (\"We love music\", \"Nous aimons la musique\"),\n",
        "    (\"She speaks French fluently\", \"Elle parle français couramment\"),\n",
        "    (\"He enjoys reading books\", \"Il aime lire des livres\"),\n",
        "    (\"They play soccer every weekend\", \"Ils jouent au football chaque week-end\"),\n",
        "    (\"The movie starts at 7 PM\", \"Le film commence à 19 heures\"),\n",
        "    (\"She wears a red dress\", \"Elle porte une robe rouge\"),\n",
        "    (\"We cook dinner together\", \"Nous cuisinons le dîner ensemble\"),\n",
        "    (\"He drives a blue car\", \"Il conduit une voiture bleue\"),\n",
        "    (\"They visit museums often\", \"Ils visitent souvent des musées\"),\n",
        "    (\"The restaurant serves delicious food\", \"Le restaurant sert une délicieuse cuisine\"),\n",
        "    (\"She studies mathematics at university\", \"Elle étudie les mathématiques à l'université\"),\n",
        "    (\"We watch movies on Fridays\", \"Nous regardons des films le vendredi\"),\n",
        "    (\"He listens to music while jogging\", \"Il écoute de la musique en faisant du jogging\"),\n",
        "    (\"They travel around the world\", \"Ils voyagent autour du monde\"),\n",
        "    (\"The book is on the table\", \"Le livre est sur la table\"),\n",
        "    (\"She dances gracefully\", \"Elle danse avec grâce\"),\n",
        "    (\"We celebrate birthdays with cake\", \"Nous célébrons les anniversaires avec un gâteau\"),\n",
        "    (\"He works hard every day\", \"Il travaille dur tous les jours\"),\n",
        "    (\"They speak different languages\", \"Ils parlent différentes langues\"),\n",
        "    (\"The flowers bloom in spring\", \"Les fleurs fleurissent au printemps\"),\n",
        "    (\"She writes poetry in her free time\", \"Elle écrit de la poésie pendant son temps libre\"),\n",
        "    (\"We learn something new every day\", \"Nous apprenons quelque chose de nouveau chaque jour\"),\n",
        "    (\"The dog barks loudly\", \"Le chien aboie bruyamment\"),\n",
        "    (\"He sings beautifully\", \"Il chante magnifiquement\"),\n",
        "    (\"They swim in the pool\", \"Ils nagent dans la piscine\"),\n",
        "    (\"The birds chirp in the morning\", \"Les oiseaux gazouillent le matin\"),\n",
        "    (\"She teaches English at school\", \"Elle enseigne l'anglais à l'école\"),\n",
        "    (\"We eat breakfast together\", \"Nous prenons le petit déjeuner ensemble\"),\n",
        "    (\"He paints landscapes\", \"Il peint des paysages\"),\n",
        "    (\"They laugh at the joke\", \"Ils rient de la blague\"),\n",
        "    (\"The clock ticks loudly\", \"L'horloge tic-tac bruyamment\"),\n",
        "    (\"She runs in the park\", \"Elle court dans le parc\"),\n",
        "    (\"We travel by train\", \"Nous voyageons en train\"),\n",
        "    (\"He writes a letter\", \"Il écrit une lettre\"),\n",
        "    (\"They read books at the library\", \"Ils lisent des livres à la bibliothèque\"),\n",
        "    (\"The baby cries\", \"Le bébé pleure\"),\n",
        "    (\"She studies hard for exams\", \"Elle étudie dur pour les examens\"),\n",
        "    (\"We plant flowers in the garden\", \"Nous plantons des fleurs dans le jardin\"),\n",
        "    (\"He fixes the car\", \"Il répare la voiture\"),\n",
        "    (\"They drink coffee in the morning\", \"Ils boivent du café le matin\"),\n",
        "    (\"The sun sets in the evening\", \"Le soleil se couche le soir\"),\n",
        "    (\"She dances at the party\", \"Elle danse à la fête\"),\n",
        "    (\"We play music at the concert\", \"Nous jouons de la musique au concert\"),\n",
        "    (\"He cooks dinner for his family\", \"Il cuisine le dîner pour sa famille\"),\n",
        "    (\"They study French grammar\", \"Ils étudient la grammaire française\"),\n",
        "    (\"The rain falls gently\", \"La pluie tombe doucement\"),\n",
        "    (\"She sings a song\", \"Elle chante une chanson\"),\n",
        "    (\"We watch a movie together\", \"Nous regardons un film ensemble\"),\n",
        "    (\"He sleeps deeply\", \"Il dort profondément\"),\n",
        "    (\"They travel to Paris\", \"Ils voyagent à Paris\"),\n",
        "    (\"The children play in the park\", \"Les enfants jouent dans le parc\"),\n",
        "    (\"She walks along the beach\", \"Elle se promène le long de la plage\"),\n",
        "    (\"We talk on the phone\", \"Nous parlons au téléphone\"),\n",
        "    (\"He waits for the bus\", \"Il attend le bus\"),\n",
        "    (\"They visit the Eiffel Tower\", \"Ils visitent la tour Eiffel\"),\n",
        "    (\"The stars twinkle at night\", \"Les étoiles scintillent la nuit\"),\n",
        "    (\"She dreams of flying\", \"Elle rêve de voler\"),\n",
        "    (\"We work in the office\", \"Nous travaillons au bureau\"),\n",
        "    (\"He studies history\", \"Il étudie l'histoire\"),\n",
        "    (\"They listen to the radio\", \"Ils écoutent la radio\"),\n",
        "    (\"The wind blows gently\", \"Le vent souffle doucement\"),\n",
        "    (\"She swims in the ocean\", \"Elle nage dans l'océan\"),\n",
        "    (\"We dance at the wedding\", \"Nous dansons au mariage\"),\n",
        "    (\"He climbs the mountain\", \"Il gravit la montagne\"),\n",
        "    (\"They hike in the forest\", \"Ils font de la randonnée dans la forêt\"),\n",
        "    (\"The cat meows loudly\", \"Le chat miaule bruyamment\"),\n",
        "    (\"She paints a picture\", \"Elle peint un tableau\"),\n",
        "    (\"We build a sandcastle\", \"Nous construisons un château de sable\"),\n",
        "    (\"He sings in the choir\", \"Il chante dans le chœur\"),\n",
        "    (\"They ride bicycles\", \"Ils font du vélo\"),\n",
        "    (\"The coffee is hot\", \"Le café est chaud\"),\n",
        "    (\"She wears glasses\", \"Elle porte des lunettes\"),\n",
        "    (\"We visit our grandparents\", \"Nous rendons visite à nos grands-parents\"),\n",
        "    (\"He plays the guitar\", \"Il joue de la guitare\"),\n",
        "    (\"They go shopping\", \"Ils font du shopping\"),\n",
        "    (\"The teacher explains the lesson\", \"Le professeur explique la leçon\"),\n",
        "    (\"She takes the train to work\", \"Elle prend le train pour aller au travail\"),\n",
        "    (\"We bake cookies\", \"Nous faisons des biscuits\"),\n",
        "    (\"He washes his hands\", \"Il se lave les mains\"),\n",
        "    (\"They enjoy the sunset\", \"Ils apprécient le coucher du soleil\"),\n",
        "    (\"The river flows calmly\", \"La rivière coule calmement\"),\n",
        "    (\"She feeds the cat\", \"Elle nourrit le chat\"),\n",
        "    (\"We visit the museum\", \"Nous visitons le musée\"),\n",
        "    (\"He fixes his bicycle\", \"Il répare son vélo\"),\n",
        "    (\"They paint the walls\", \"Ils peignent les murs\"),\n",
        "    (\"The baby sleeps peacefully\", \"Le bébé dort paisiblement\"),\n",
        "    (\"She ties her shoelaces\", \"Elle attache ses lacets\"),\n",
        "    (\"We climb the stairs\", \"Nous montons les escaliers\"),\n",
        "    (\"He shaves in the morning\", \"Il se rase le matin\"),\n",
        "    (\"They set the table\", \"Ils mettent la table\"),\n",
        "    (\"The airplane takes off\", \"L'avion décolle\"),\n",
        "    (\"She waters the plants\", \"Elle arrose les plantes\"),\n",
        "    (\"We practice yoga\", \"Nous pratiquons le yoga\"),\n",
        "    (\"He turns off the light\", \"Il éteint la lumière\"),\n",
        "    (\"They play video games\", \"Ils jouent aux jeux vidéo\"),\n",
        "    (\"The soup smells delicious\", \"La soupe sent délicieusement bon\"),\n",
        "    (\"She locks the door\", \"Elle ferme la porte à clé\"),\n",
        "    (\"We enjoy a picnic\", \"Nous profitons d'un pique-nique\"),\n",
        "    (\"He checks his email\", \"Il vérifie ses emails\"),\n",
        "    (\"They go to the gym\", \"Ils vont à la salle de sport\"),\n",
        "    (\"The moon shines brightly\", \"La lune brille intensément\"),\n",
        "    (\"She catches the bus\", \"Elle attrape le bus\"),\n",
        "    (\"We greet our neighbors\", \"Nous saluons nos voisins\"),\n",
        "    (\"He combs his hair\", \"Il se peigne les cheveux\"),\n",
        "    (\"They wave goodbye\", \"Ils font un signe d'adieu\")\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ========= DATA =========\n",
        "\n",
        "pairs_fr_en = [(fr, en) for en, fr in pairs_en_fr]\n",
        "\n",
        "# ========= VOCAB =========\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def get_vocabs(pairs):\n",
        "    src_sentences, tgt_sentences = zip(*pairs)\n",
        "    src_vocab = build_vocab(src_sentences)\n",
        "    tgt_vocab = build_vocab(tgt_sentences)\n",
        "    inv_tgt_vocab = {i: w for w, i in tgt_vocab.items()}\n",
        "    return src_vocab, tgt_vocab, inv_tgt_vocab\n",
        "\n",
        "# ========= DATASET =========\n",
        "\n",
        "def tokenize(sentence, vocab):\n",
        "    return [vocab.get(word, vocab[\"<unk>\"]) for word in sentence.lower().split()]\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
        "        self.data = pairs\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        src_ids = tokenize(src, self.src_vocab)\n",
        "        tgt_ids = [self.tgt_vocab[\"<sos>\"]] + tokenize(tgt, self.tgt_vocab) + [self.tgt_vocab[\"<eos>\"]]\n",
        "        return {\n",
        "            \"src\": torch.tensor(src_ids, dtype=torch.long),\n",
        "            \"tgt\": torch.tensor(tgt_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch = [item[\"src\"] for item in batch]\n",
        "    tgt_batch = [item[\"tgt\"] for item in batch]\n",
        "    return (\n",
        "        pad_sequence(src_batch, padding_value=0, batch_first=True),\n",
        "        pad_sequence(tgt_batch, padding_value=0, batch_first=True)\n",
        "    )\n",
        "\n",
        "# ========= MODEL COMPONENTS =========\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, padding_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=padding_idx)\n",
        "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2], hidden[-1]), dim=1)))\n",
        "        return outputs, hidden.unsqueeze(0)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        batch_size, src_len, _ = encoder_outputs.shape\n",
        "        hidden = hidden.permute(1, 0, 2).repeat(1, src_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, padding_idx, attention=None):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=padding_idx)\n",
        "        self.attention = attention\n",
        "        input_dim = hidden_dim * 2 + emb_dim if attention else hidden_dim + emb_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim) if attention is None else nn.Linear(hidden_dim * 3 + emb_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs=None, mask=None):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        if self.attention:\n",
        "            attn_weights = self.attention(hidden, encoder_outputs, mask).unsqueeze(1)\n",
        "            context = torch.bmm(attn_weights, encoder_outputs)\n",
        "            rnn_input = torch.cat((embedded, context), dim=2)\n",
        "            output, hidden = self.gru(rnn_input, hidden)\n",
        "            output = output.squeeze(1)\n",
        "            context = context.squeeze(1)\n",
        "            embedded = embedded.squeeze(1)\n",
        "            prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n",
        "        else:\n",
        "            rnn_input = torch.cat((embedded, hidden.permute(1, 0, 2)), dim=2)\n",
        "            output, hidden = self.gru(rnn_input, hidden)\n",
        "            prediction = self.fc_out(output.squeeze(1))\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, src):\n",
        "        return (src != 0)\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "        tgt_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        input = tgt[:, 0]\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs if self.decoder.attention else None, mask if self.decoder.attention else None)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# ========= TRAINING, EVAL, ACCURACY =========\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output_flat = output[:, 1:].reshape(-1, output_dim)\n",
        "            tgt_flat = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output_flat, tgt_flat)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predictions = output_flat.argmax(1)\n",
        "            mask = tgt_flat != 0\n",
        "            total_correct += (predictions == tgt_flat)[mask].sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "# ========= EXPERIMENT FUNCTION =========\n",
        "\n",
        "def run_experiment(name, pairs, use_attention, epochs=20):\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "\n",
        "    src_vocab, tgt_vocab, inv_tgt_vocab = get_vocabs(pairs)\n",
        "\n",
        "    dataset = TranslationDataset(pairs, src_vocab, tgt_vocab)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
        "\n",
        "    encoder = Encoder(len(src_vocab), emb_dim=32, hidden_dim=64, padding_idx=src_vocab[\"<pad>\"])\n",
        "    attention = Attention(64) if use_attention else None\n",
        "    decoder = Decoder(len(tgt_vocab), emb_dim=32, hidden_dim=64, padding_idx=tgt_vocab[\"<pad>\"], attention=attention)\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train(model, dataloader, optimizer, criterion)\n",
        "        val_loss, val_acc = evaluate(model, dataloader, criterion)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "    return history, model\n",
        "\n",
        "# ========= RUN ALL EXPERIMENTS =========\n",
        "\n",
        "histories = {}\n",
        "\n",
        "histories[\"EN->FR (No Attention)\"], model1  = run_experiment(\"EN->FR (No Attention)\", pairs_en_fr, use_attention=False)\n",
        "histories[\"EN->FR (With Attention)\"], model2 = run_experiment(\"EN->FR (With Attention)\", pairs_en_fr, use_attention=True)\n",
        "histories[\"FR->EN (No Attention)\"], model3 = run_experiment(\"FR->EN (No Attention)\", pairs_fr_en, use_attention=False)\n",
        "histories[\"FR->EN (With Attention)\"], model4 = run_experiment(\"FR->EN (With Attention)\", pairs_fr_en, use_attention=True)\n",
        "\n",
        "# ========= DISPLAY RESULTS =========\n",
        "\n",
        "for model_name, hist in histories.items():\n",
        "    print(f\"\\n=== Results for {model_name} ===\")\n",
        "    for epoch, (tr, vl, acc) in enumerate(zip(hist[\"train_loss\"], hist[\"val_loss\"], hist[\"val_acc\"]), 1):\n",
        "        print(f\"Epoch {epoch}: Train Loss = {tr:.4f}, Val Loss = {vl:.4f}, Val Acc = {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MGabwVOAcx6",
        "outputId": "4f90fd9f-787b-4289-cda1-7394d7df1a1a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training EN->FR (No Attention) ===\n",
            "Epoch 1: Train Loss = 5.1120, Val Loss = 4.3196, Val Acc = 0.2258\n",
            "Epoch 2: Train Loss = 4.1576, Val Loss = 3.8828, Val Acc = 0.3134\n",
            "Epoch 3: Train Loss = 3.8175, Val Loss = 3.6188, Val Acc = 0.3456\n",
            "Epoch 4: Train Loss = 3.6063, Val Loss = 3.4363, Val Acc = 0.3487\n",
            "Epoch 5: Train Loss = 3.4315, Val Loss = 3.2887, Val Acc = 0.3564\n",
            "Epoch 6: Train Loss = 3.2787, Val Loss = 3.1681, Val Acc = 0.3717\n",
            "Epoch 7: Train Loss = 3.1374, Val Loss = 3.0276, Val Acc = 0.3687\n",
            "Epoch 8: Train Loss = 3.0058, Val Loss = 2.8993, Val Acc = 0.3763\n",
            "Epoch 9: Train Loss = 2.8795, Val Loss = 2.7679, Val Acc = 0.3840\n",
            "Epoch 10: Train Loss = 2.7524, Val Loss = 2.6465, Val Acc = 0.3994\n",
            "Epoch 11: Train Loss = 2.6456, Val Loss = 2.5360, Val Acc = 0.4147\n",
            "Epoch 12: Train Loss = 2.5415, Val Loss = 2.4512, Val Acc = 0.4255\n",
            "Epoch 13: Train Loss = 2.4351, Val Loss = 2.3759, Val Acc = 0.4455\n",
            "Epoch 14: Train Loss = 2.3470, Val Loss = 2.2687, Val Acc = 0.4654\n",
            "Epoch 15: Train Loss = 2.2401, Val Loss = 2.1713, Val Acc = 0.4700\n",
            "Epoch 16: Train Loss = 2.1279, Val Loss = 2.0795, Val Acc = 0.4946\n",
            "Epoch 17: Train Loss = 2.0311, Val Loss = 1.9922, Val Acc = 0.5115\n",
            "Epoch 18: Train Loss = 1.9554, Val Loss = 1.9245, Val Acc = 0.5084\n",
            "Epoch 19: Train Loss = 1.8824, Val Loss = 1.8450, Val Acc = 0.5315\n",
            "Epoch 20: Train Loss = 1.8052, Val Loss = 1.7730, Val Acc = 0.5545\n",
            "\n",
            "=== Training EN->FR (With Attention) ===\n",
            "Epoch 1: Train Loss = 5.1400, Val Loss = 4.1316, Val Acc = 0.2888\n",
            "Epoch 2: Train Loss = 3.9837, Val Loss = 3.5761, Val Acc = 0.3287\n",
            "Epoch 3: Train Loss = 3.4888, Val Loss = 3.1493, Val Acc = 0.3502\n",
            "Epoch 4: Train Loss = 3.0803, Val Loss = 2.7613, Val Acc = 0.3871\n",
            "Epoch 5: Train Loss = 2.6896, Val Loss = 2.3929, Val Acc = 0.4255\n",
            "Epoch 6: Train Loss = 2.3229, Val Loss = 2.0206, Val Acc = 0.4946\n",
            "Epoch 7: Train Loss = 1.9355, Val Loss = 1.6573, Val Acc = 0.6037\n",
            "Epoch 8: Train Loss = 1.6210, Val Loss = 1.3332, Val Acc = 0.6882\n",
            "Epoch 9: Train Loss = 1.2621, Val Loss = 1.1288, Val Acc = 0.7081\n",
            "Epoch 10: Train Loss = 1.0087, Val Loss = 0.8790, Val Acc = 0.7849\n",
            "Epoch 11: Train Loss = 0.8128, Val Loss = 0.7193, Val Acc = 0.8464\n",
            "Epoch 12: Train Loss = 0.6478, Val Loss = 0.6154, Val Acc = 0.8571\n",
            "Epoch 13: Train Loss = 0.5328, Val Loss = 0.4933, Val Acc = 0.9140\n",
            "Epoch 14: Train Loss = 0.4070, Val Loss = 0.4181, Val Acc = 0.9370\n",
            "Epoch 15: Train Loss = 0.3160, Val Loss = 0.2717, Val Acc = 0.9662\n",
            "Epoch 16: Train Loss = 0.2643, Val Loss = 0.2227, Val Acc = 0.9724\n",
            "Epoch 17: Train Loss = 0.2093, Val Loss = 0.1806, Val Acc = 0.9770\n",
            "Epoch 18: Train Loss = 0.1684, Val Loss = 0.1395, Val Acc = 0.9939\n",
            "Epoch 19: Train Loss = 0.1422, Val Loss = 0.1174, Val Acc = 0.9985\n",
            "Epoch 20: Train Loss = 0.1165, Val Loss = 0.0973, Val Acc = 0.9985\n",
            "\n",
            "=== Training FR->EN (No Attention) ===\n",
            "Epoch 1: Train Loss = 5.0461, Val Loss = 4.2004, Val Acc = 0.2982\n",
            "Epoch 2: Train Loss = 3.9627, Val Loss = 3.6517, Val Acc = 0.3888\n",
            "Epoch 3: Train Loss = 3.5457, Val Loss = 3.3546, Val Acc = 0.3921\n",
            "Epoch 4: Train Loss = 3.2935, Val Loss = 3.1456, Val Acc = 0.4036\n",
            "Epoch 5: Train Loss = 3.1242, Val Loss = 3.0018, Val Acc = 0.4036\n",
            "Epoch 6: Train Loss = 2.9504, Val Loss = 2.9095, Val Acc = 0.4119\n",
            "Epoch 7: Train Loss = 2.8355, Val Loss = 2.7988, Val Acc = 0.4283\n",
            "Epoch 8: Train Loss = 2.7066, Val Loss = 2.6438, Val Acc = 0.4432\n",
            "Epoch 9: Train Loss = 2.5876, Val Loss = 2.4770, Val Acc = 0.4629\n",
            "Epoch 10: Train Loss = 2.4410, Val Loss = 2.3299, Val Acc = 0.4860\n",
            "Epoch 11: Train Loss = 2.2942, Val Loss = 2.2597, Val Acc = 0.4893\n",
            "Epoch 12: Train Loss = 2.2089, Val Loss = 2.2352, Val Acc = 0.4975\n",
            "Epoch 13: Train Loss = 2.1377, Val Loss = 2.0432, Val Acc = 0.5288\n",
            "Epoch 14: Train Loss = 2.0146, Val Loss = 1.9150, Val Acc = 0.5519\n",
            "Epoch 15: Train Loss = 1.8901, Val Loss = 1.8259, Val Acc = 0.5651\n",
            "Epoch 16: Train Loss = 1.7825, Val Loss = 1.7127, Val Acc = 0.5733\n",
            "Epoch 17: Train Loss = 1.6928, Val Loss = 1.6331, Val Acc = 0.5931\n",
            "Epoch 18: Train Loss = 1.5978, Val Loss = 1.5545, Val Acc = 0.6145\n",
            "Epoch 19: Train Loss = 1.5168, Val Loss = 1.4800, Val Acc = 0.6260\n",
            "Epoch 20: Train Loss = 1.4512, Val Loss = 1.3931, Val Acc = 0.6755\n",
            "\n",
            "=== Training FR->EN (With Attention) ===\n",
            "Epoch 1: Train Loss = 4.9390, Val Loss = 3.9255, Val Acc = 0.2801\n",
            "Epoch 2: Train Loss = 3.7245, Val Loss = 3.3357, Val Acc = 0.3756\n",
            "Epoch 3: Train Loss = 3.2173, Val Loss = 2.9351, Val Acc = 0.4069\n",
            "Epoch 4: Train Loss = 2.8492, Val Loss = 2.5957, Val Acc = 0.4316\n",
            "Epoch 5: Train Loss = 2.5352, Val Loss = 2.3060, Val Acc = 0.4596\n",
            "Epoch 6: Train Loss = 2.2035, Val Loss = 2.0601, Val Acc = 0.4992\n",
            "Epoch 7: Train Loss = 1.8632, Val Loss = 1.7048, Val Acc = 0.5750\n",
            "Epoch 8: Train Loss = 1.5610, Val Loss = 1.4041, Val Acc = 0.6540\n",
            "Epoch 9: Train Loss = 1.2643, Val Loss = 1.0954, Val Acc = 0.7694\n",
            "Epoch 10: Train Loss = 1.0058, Val Loss = 0.7757, Val Acc = 0.8781\n",
            "Epoch 11: Train Loss = 0.7713, Val Loss = 0.6365, Val Acc = 0.9143\n",
            "Epoch 12: Train Loss = 0.5891, Val Loss = 0.5507, Val Acc = 0.9143\n",
            "Epoch 13: Train Loss = 0.4675, Val Loss = 0.3736, Val Acc = 0.9588\n",
            "Epoch 14: Train Loss = 0.3540, Val Loss = 0.2738, Val Acc = 0.9671\n",
            "Epoch 15: Train Loss = 0.2638, Val Loss = 0.1966, Val Acc = 0.9951\n",
            "Epoch 16: Train Loss = 0.2004, Val Loss = 0.1700, Val Acc = 0.9885\n",
            "Epoch 17: Train Loss = 0.1722, Val Loss = 0.1316, Val Acc = 1.0000\n",
            "Epoch 18: Train Loss = 0.1468, Val Loss = 0.1349, Val Acc = 0.9885\n",
            "Epoch 19: Train Loss = 0.1249, Val Loss = 0.0981, Val Acc = 0.9984\n",
            "Epoch 20: Train Loss = 0.0994, Val Loss = 0.0826, Val Acc = 1.0000\n",
            "\n",
            "=== Results for EN->FR (No Attention) ===\n",
            "Epoch 1: Train Loss = 5.1120, Val Loss = 4.3196, Val Acc = 0.2258\n",
            "Epoch 2: Train Loss = 4.1576, Val Loss = 3.8828, Val Acc = 0.3134\n",
            "Epoch 3: Train Loss = 3.8175, Val Loss = 3.6188, Val Acc = 0.3456\n",
            "Epoch 4: Train Loss = 3.6063, Val Loss = 3.4363, Val Acc = 0.3487\n",
            "Epoch 5: Train Loss = 3.4315, Val Loss = 3.2887, Val Acc = 0.3564\n",
            "Epoch 6: Train Loss = 3.2787, Val Loss = 3.1681, Val Acc = 0.3717\n",
            "Epoch 7: Train Loss = 3.1374, Val Loss = 3.0276, Val Acc = 0.3687\n",
            "Epoch 8: Train Loss = 3.0058, Val Loss = 2.8993, Val Acc = 0.3763\n",
            "Epoch 9: Train Loss = 2.8795, Val Loss = 2.7679, Val Acc = 0.3840\n",
            "Epoch 10: Train Loss = 2.7524, Val Loss = 2.6465, Val Acc = 0.3994\n",
            "Epoch 11: Train Loss = 2.6456, Val Loss = 2.5360, Val Acc = 0.4147\n",
            "Epoch 12: Train Loss = 2.5415, Val Loss = 2.4512, Val Acc = 0.4255\n",
            "Epoch 13: Train Loss = 2.4351, Val Loss = 2.3759, Val Acc = 0.4455\n",
            "Epoch 14: Train Loss = 2.3470, Val Loss = 2.2687, Val Acc = 0.4654\n",
            "Epoch 15: Train Loss = 2.2401, Val Loss = 2.1713, Val Acc = 0.4700\n",
            "Epoch 16: Train Loss = 2.1279, Val Loss = 2.0795, Val Acc = 0.4946\n",
            "Epoch 17: Train Loss = 2.0311, Val Loss = 1.9922, Val Acc = 0.5115\n",
            "Epoch 18: Train Loss = 1.9554, Val Loss = 1.9245, Val Acc = 0.5084\n",
            "Epoch 19: Train Loss = 1.8824, Val Loss = 1.8450, Val Acc = 0.5315\n",
            "Epoch 20: Train Loss = 1.8052, Val Loss = 1.7730, Val Acc = 0.5545\n",
            "\n",
            "=== Results for EN->FR (With Attention) ===\n",
            "Epoch 1: Train Loss = 5.1400, Val Loss = 4.1316, Val Acc = 0.2888\n",
            "Epoch 2: Train Loss = 3.9837, Val Loss = 3.5761, Val Acc = 0.3287\n",
            "Epoch 3: Train Loss = 3.4888, Val Loss = 3.1493, Val Acc = 0.3502\n",
            "Epoch 4: Train Loss = 3.0803, Val Loss = 2.7613, Val Acc = 0.3871\n",
            "Epoch 5: Train Loss = 2.6896, Val Loss = 2.3929, Val Acc = 0.4255\n",
            "Epoch 6: Train Loss = 2.3229, Val Loss = 2.0206, Val Acc = 0.4946\n",
            "Epoch 7: Train Loss = 1.9355, Val Loss = 1.6573, Val Acc = 0.6037\n",
            "Epoch 8: Train Loss = 1.6210, Val Loss = 1.3332, Val Acc = 0.6882\n",
            "Epoch 9: Train Loss = 1.2621, Val Loss = 1.1288, Val Acc = 0.7081\n",
            "Epoch 10: Train Loss = 1.0087, Val Loss = 0.8790, Val Acc = 0.7849\n",
            "Epoch 11: Train Loss = 0.8128, Val Loss = 0.7193, Val Acc = 0.8464\n",
            "Epoch 12: Train Loss = 0.6478, Val Loss = 0.6154, Val Acc = 0.8571\n",
            "Epoch 13: Train Loss = 0.5328, Val Loss = 0.4933, Val Acc = 0.9140\n",
            "Epoch 14: Train Loss = 0.4070, Val Loss = 0.4181, Val Acc = 0.9370\n",
            "Epoch 15: Train Loss = 0.3160, Val Loss = 0.2717, Val Acc = 0.9662\n",
            "Epoch 16: Train Loss = 0.2643, Val Loss = 0.2227, Val Acc = 0.9724\n",
            "Epoch 17: Train Loss = 0.2093, Val Loss = 0.1806, Val Acc = 0.9770\n",
            "Epoch 18: Train Loss = 0.1684, Val Loss = 0.1395, Val Acc = 0.9939\n",
            "Epoch 19: Train Loss = 0.1422, Val Loss = 0.1174, Val Acc = 0.9985\n",
            "Epoch 20: Train Loss = 0.1165, Val Loss = 0.0973, Val Acc = 0.9985\n",
            "\n",
            "=== Results for FR->EN (No Attention) ===\n",
            "Epoch 1: Train Loss = 5.0461, Val Loss = 4.2004, Val Acc = 0.2982\n",
            "Epoch 2: Train Loss = 3.9627, Val Loss = 3.6517, Val Acc = 0.3888\n",
            "Epoch 3: Train Loss = 3.5457, Val Loss = 3.3546, Val Acc = 0.3921\n",
            "Epoch 4: Train Loss = 3.2935, Val Loss = 3.1456, Val Acc = 0.4036\n",
            "Epoch 5: Train Loss = 3.1242, Val Loss = 3.0018, Val Acc = 0.4036\n",
            "Epoch 6: Train Loss = 2.9504, Val Loss = 2.9095, Val Acc = 0.4119\n",
            "Epoch 7: Train Loss = 2.8355, Val Loss = 2.7988, Val Acc = 0.4283\n",
            "Epoch 8: Train Loss = 2.7066, Val Loss = 2.6438, Val Acc = 0.4432\n",
            "Epoch 9: Train Loss = 2.5876, Val Loss = 2.4770, Val Acc = 0.4629\n",
            "Epoch 10: Train Loss = 2.4410, Val Loss = 2.3299, Val Acc = 0.4860\n",
            "Epoch 11: Train Loss = 2.2942, Val Loss = 2.2597, Val Acc = 0.4893\n",
            "Epoch 12: Train Loss = 2.2089, Val Loss = 2.2352, Val Acc = 0.4975\n",
            "Epoch 13: Train Loss = 2.1377, Val Loss = 2.0432, Val Acc = 0.5288\n",
            "Epoch 14: Train Loss = 2.0146, Val Loss = 1.9150, Val Acc = 0.5519\n",
            "Epoch 15: Train Loss = 1.8901, Val Loss = 1.8259, Val Acc = 0.5651\n",
            "Epoch 16: Train Loss = 1.7825, Val Loss = 1.7127, Val Acc = 0.5733\n",
            "Epoch 17: Train Loss = 1.6928, Val Loss = 1.6331, Val Acc = 0.5931\n",
            "Epoch 18: Train Loss = 1.5978, Val Loss = 1.5545, Val Acc = 0.6145\n",
            "Epoch 19: Train Loss = 1.5168, Val Loss = 1.4800, Val Acc = 0.6260\n",
            "Epoch 20: Train Loss = 1.4512, Val Loss = 1.3931, Val Acc = 0.6755\n",
            "\n",
            "=== Results for FR->EN (With Attention) ===\n",
            "Epoch 1: Train Loss = 4.9390, Val Loss = 3.9255, Val Acc = 0.2801\n",
            "Epoch 2: Train Loss = 3.7245, Val Loss = 3.3357, Val Acc = 0.3756\n",
            "Epoch 3: Train Loss = 3.2173, Val Loss = 2.9351, Val Acc = 0.4069\n",
            "Epoch 4: Train Loss = 2.8492, Val Loss = 2.5957, Val Acc = 0.4316\n",
            "Epoch 5: Train Loss = 2.5352, Val Loss = 2.3060, Val Acc = 0.4596\n",
            "Epoch 6: Train Loss = 2.2035, Val Loss = 2.0601, Val Acc = 0.4992\n",
            "Epoch 7: Train Loss = 1.8632, Val Loss = 1.7048, Val Acc = 0.5750\n",
            "Epoch 8: Train Loss = 1.5610, Val Loss = 1.4041, Val Acc = 0.6540\n",
            "Epoch 9: Train Loss = 1.2643, Val Loss = 1.0954, Val Acc = 0.7694\n",
            "Epoch 10: Train Loss = 1.0058, Val Loss = 0.7757, Val Acc = 0.8781\n",
            "Epoch 11: Train Loss = 0.7713, Val Loss = 0.6365, Val Acc = 0.9143\n",
            "Epoch 12: Train Loss = 0.5891, Val Loss = 0.5507, Val Acc = 0.9143\n",
            "Epoch 13: Train Loss = 0.4675, Val Loss = 0.3736, Val Acc = 0.9588\n",
            "Epoch 14: Train Loss = 0.3540, Val Loss = 0.2738, Val Acc = 0.9671\n",
            "Epoch 15: Train Loss = 0.2638, Val Loss = 0.1966, Val Acc = 0.9951\n",
            "Epoch 16: Train Loss = 0.2004, Val Loss = 0.1700, Val Acc = 0.9885\n",
            "Epoch 17: Train Loss = 0.1722, Val Loss = 0.1316, Val Acc = 1.0000\n",
            "Epoch 18: Train Loss = 0.1468, Val Loss = 0.1349, Val Acc = 0.9885\n",
            "Epoch 19: Train Loss = 0.1249, Val Loss = 0.0981, Val Acc = 0.9984\n",
            "Epoch 20: Train Loss = 0.0994, Val Loss = 0.0826, Val Acc = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_sentences = [\n",
        "    \"I am a student\",\n",
        "    \"The weather is nice today\",\n",
        "    \"The car is red\",\n",
        "    \"We love learning programming\"\n",
        "]\n",
        "fr_sentences = [\n",
        "    \"je suis étudiant\",\n",
        "    \"il fait beau aujourd'hui\",\n",
        "    \"la voiture est rouge\",\n",
        "    \"nous aimons apprendre la programmation\"\n",
        "]\n",
        "def evaluate_models_direction(models, sentences, src_vocab, tgt_vocab, inv_tgt_vocab, description):\n",
        "    print(f\"\\n==== {description} ====\")\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        print(f\"\\n Source: '{sentence}'\")\n",
        "        for i, model in enumerate(models, 1):\n",
        "            translation = translate_sentence(model, sentence, src_vocab, tgt_vocab, inv_tgt_vocab)\n",
        "            if i%2==0:\n",
        "              name = \"with attention\"\n",
        "            else:\n",
        "              name = \"no attention\"\n",
        "            print(f\"GRU {name} Translation: {' '.join(translation)}\")\n",
        "evaluate_models_direction(\n",
        "    [model1, model2],\n",
        "    en_sentences,\n",
        "    en_vocab, fr_vocab, inv_fr_vocab,\n",
        "    \"English to French Translation\"\n",
        ")\n",
        "\n",
        "evaluate_models_direction(\n",
        "    [model3, model4],\n",
        "    fr_sentences,\n",
        "    fr_vocab, en_vocab, inv_en_vocab,\n",
        "    \"French to English Translation\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dpT6N_qGbu0",
        "outputId": "2dadbca4-d4b7-48f3-fbbf-061eb0ce09bd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== English to French Translation ====\n",
            "\n",
            " Source: 'I am a student'\n",
            "GRU no attention Translation: j'ai froid\n",
            "GRU with attention Translation: ils font est chaud\n",
            "\n",
            " Source: 'The weather is nice today'\n",
            "GRU no attention Translation: le chien dort\n",
            "GRU with attention Translation: le chat est chaud\n",
            "\n",
            " Source: 'The car is red'\n",
            "GRU no attention Translation: le bébé est chaud\n",
            "GRU with attention Translation: le soleil brille\n",
            "\n",
            " Source: 'We love learning programming'\n",
            "GRU no attention Translation: nous aimons la\n",
            "GRU with attention Translation: nous aimons le le\n",
            "\n",
            "==== French to English Translation ====\n",
            "\n",
            " Source: 'je suis étudiant'\n",
            "GRU no attention Translation: she studies a flying\n",
            "GRU with attention Translation: the cat in the bus\n",
            "\n",
            " Source: 'il fait beau aujourd'hui'\n",
            "GRU no attention Translation: he sings beautifully\n",
            "GRU with attention Translation: he waits for the bus\n",
            "\n",
            " Source: 'la voiture est rouge'\n",
            "GRU no attention Translation: the airplane takes at\n",
            "GRU with attention Translation: the rain falls music\n",
            "\n",
            " Source: 'nous aimons apprendre la programmation'\n",
            "GRU no attention Translation: we visit our grandparents\n",
            "GRU with attention Translation: we climb the stairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hux2_9WGsI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}